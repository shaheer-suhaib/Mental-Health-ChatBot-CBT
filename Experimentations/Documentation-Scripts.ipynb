{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "895ae235",
   "metadata": {},
   "source": [
    "# with langgrpah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f4ac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from dataclasses import dataclass\n",
    "from langgraph.graph import StateGraph, END\n",
    "#from langgraph.utils.input_annotations import add_messages\n",
    "from operator import add as add_messages\n",
    "\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Define node: simulate LLM replying to the last human message\n",
    "def respond_node(state: State) -> dict:\n",
    "    last_msg = state[\"messages\"][-1].content \n",
    "    reply = f\"Echo: {last_msg}\"             \n",
    "    new_ai_msg = AIMessage(content=reply)\n",
    "    return {\"messages\": [new_ai_msg]}         \n",
    "\n",
    "# Build graph\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(\"respond\", respond_node)\n",
    "    .set_entry_point(\"respond\")\n",
    "    .add_edge(\"respond\", END)\n",
    "    .compile()\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9ba8d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<langgraph.graph.state.CompiledStateGraph object at 0x0000022B40247410>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94196952",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# First run — with one user message\n",
    "state1 = {\n",
    "    \"topic\": \"chat\",\n",
    "    \"messages\": [HumanMessage(content=\"Hello, who are you?\")]\n",
    "}\n",
    "result1 = graph.invoke(state1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c7e277c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'chat',\n",
       " 'messages': [HumanMessage(content='Hello, who are you?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Echo: Hello, who are you?', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50bf986a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hello, who are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Echo: Hello, who are you?', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1[\"messages\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0cde4e66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== After First Call ===\n",
      "HumanMessage: Hello, who are you?\n",
      "AIMessage: Echo: Hello, who are you?\n"
     ]
    }
   ],
   "source": [
    "print(\"=== After First Call ===\")\n",
    "for m in result1[\"messages\"]:\n",
    "    print(f\"{m.__class__.__name__}: {m.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "44c24cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Second run — send back the updated message list\n",
    "state2 = {\n",
    "    \"topic\": \"chat\",\n",
    "    \"messages\":  [HumanMessage(content=\"Tell me a joke.\")]\n",
    "}\n",
    "result2 = graph.invoke(state2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "88e9686c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'topic': 'chat',\n",
       " 'messages': [HumanMessage(content='Tell me a joke.', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Echo: Tell me a joke.', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e6d422ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with  CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a02914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After first message:\n",
      "  HumanMessage: Hello, who are you?\n",
      "  AIMessage: Echo: Hello, who are you?\n",
      "\n",
      "After second message:\n",
      "  HumanMessage: Hello, who are you?\n",
      "  AIMessage: Echo: Hello, who are you?\n",
      "  HumanMessage: Tell me a joke.\n",
      "  AIMessage: Echo: Tell me a joke.\n",
      "\n",
      "After third message:\n",
      "  HumanMessage: Hello, who are you?\n",
      "  AIMessage: Echo: Hello, who are you?\n",
      "  HumanMessage: Tell me a joke.\n",
      "  AIMessage: Echo: Tell me a joke.\n",
      "  HumanMessage: What's the weather like?\n",
      "  AIMessage: Echo: What's the weather like?\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from operator import add as add_messages\n",
    "\n",
    "# Define the state - add_messages will handle accumulation\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "# Define node: simulate LLM replying to the last human message\n",
    "def respond_node(state: State) -> dict:\n",
    "    last_msg = state[\"messages\"][-1].content  \n",
    "    reply = f\"Echo: {last_msg}\"             \n",
    "    new_ai_msg = AIMessage(content=reply)\n",
    "    return {\"messages\": [new_ai_msg]}        \n",
    "\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(\"respond\", respond_node)\n",
    "    .set_entry_point(\"respond\")\n",
    "    .add_edge(\"respond\", END)\n",
    "    .compile(checkpointer=checkpointer)\n",
    ")\n",
    "\n",
    "# Now add_messages works its magic! ✨\n",
    "config = {\"configurable\": {\"thread_id\": \"conversation-1\"}}\n",
    "\n",
    "# First run - just send the new message\n",
    "result1 = graph.invoke({\n",
    "    \"topic\": \"chat\",\n",
    "    \"messages\": [HumanMessage(content=\"Hello, who are you?\")]\n",
    "},config=config)\n",
    "\n",
    "print(\"After first message:\")\n",
    "for msg in result1[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "# Second run - just send the NEW message, add_messages handles the rest!\n",
    "result2 = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Tell me a joke.\")]\n",
    "},config=config)\n",
    "\n",
    "print(\"\\nAfter second message:\")\n",
    "for msg in result2[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "# Third run - keep the conversation going\n",
    "result3 = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"What's the weather like?\")]\n",
    "},config=config)\n",
    "\n",
    "print(\"\\nAfter third message:\")\n",
    "for msg in result3[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b183075d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "New conversation thread:\n",
      "  HumanMessage: Start fresh conversation\n",
      "  AIMessage: Echo: Start fresh conversation\n"
     ]
    }
   ],
   "source": [
    "# Different conversation thread\n",
    "config2 = {\"configurable\": {\"thread_id\": \"conversation-2\"}}\n",
    "result4 = graph.invoke({\n",
    "    \"topic\": \"different_chat\",\n",
    "    \"messages\": [HumanMessage(content=\"Start fresh conversation\")]\n",
    "}, config2)\n",
    "\n",
    "print(\"\\nNew conversation thread:\")\n",
    "for msg in result4[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf4cb66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== WITHOUT CHECKPOINTER ===\n",
      "After first message:\n",
      "  HumanMessage: Hello, who are you?\n",
      "  AIMessage: Echo: Hello, who are you?\n",
      "\n",
      "After second message (previous messages LOST!):\n",
      "  HumanMessage: Tell me a joke.\n",
      "  AIMessage: Echo: Tell me a joke.\n",
      "\n",
      "==================================================\n",
      "SOLUTION: You need to manually maintain history\n",
      "==================================================\n",
      "\n",
      "With manual history management:\n",
      "  HumanMessage: Hello, who are you?\n",
      "  AIMessage: Echo: Hello, who are you?\n",
      "  HumanMessage: What's the weather?\n",
      "  AIMessage: Echo: What's the weather?\n"
     ]
    }
   ],
   "source": [
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
    "from langgraph.graph import StateGraph, END\n",
    "from operator import add as add_messages\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "\n",
    "def respond_node(state: State) -> dict:\n",
    "    last_msg = state[\"messages\"][-1].content\n",
    "    reply = f\"Echo: {last_msg}\"\n",
    "    new_ai_msg = AIMessage(content=reply)\n",
    "    return {\"messages\": [new_ai_msg]}  # add_messages works HERE within the execution\n",
    "\n",
    "# Compile without checkpointer\n",
    "graph = (\n",
    "    StateGraph(State)\n",
    "    .add_node(\"respond\", respond_node)\n",
    "    .set_entry_point(\"respond\")\n",
    "    .add_edge(\"respond\", END)\n",
    "    .compile()  # Fixed: Added compile()\n",
    ")\n",
    "\n",
    "print(\"=== WITHOUT CHECKPOINTER ===\")\n",
    "\n",
    "# First run - add_messages works WITHIN this execution\n",
    "result1 = graph.invoke({\n",
    "    \"topic\": \"chat\",\n",
    "    \"messages\": [HumanMessage(content=\"Hello, who are you?\")]\n",
    "})\n",
    "\n",
    "print(\"After first message:\")\n",
    "for msg in result1[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "# Second run - NO MEMORY! Previous messages are lost\n",
    "result2 = graph.invoke({\n",
    "    \"messages\": [HumanMessage(content=\"Tell me a joke.\")]\n",
    "})\n",
    "\n",
    "print(\"\\nAfter second message (previous messages LOST!):\")\n",
    "for msg in result2[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SOLUTION: You need to manually maintain history\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Manual approach (what you were doing originally)\n",
    "result3 = graph.invoke({\n",
    "    \"topic\": \"chat\",\n",
    "    \"messages\": result1[\"messages\"] + [HumanMessage(content=\"What's the weather?\")]\n",
    "})\n",
    "\n",
    "print(\"\\nWith manual history management:\")\n",
    "for msg in result3[\"messages\"]:\n",
    "    print(f\"  {type(msg).__name__}: {msg.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c801a16d",
   "metadata": {},
   "source": [
    "# json st stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed92ce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import AIMessage\n",
    "\n",
    "from google.generativeai.types import GenerationConfig\n",
    "\n",
    "\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEYJ\")\n",
    "\n",
    "\n",
    "llm_json = genai.GenerativeModel(\n",
    "    model_name=\"gemini-2.0-flash-lite\",\n",
    "    generation_config=GenerationConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        temperature=0\n",
    "    )\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61595600",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "genai.GenerativeModel(\n",
       "    model_name='models/gemini-2.0-flash-lite',\n",
       "    generation_config={'temperature': 0, 'response_mime_type': 'application/json'},\n",
       "    safety_settings={},\n",
       "    tools=None,\n",
       "    system_instruction=None,\n",
       "    cached_content=None\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b77a00f4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'google.generativeai.generative_models.GenerativeModel'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m      2\u001b[0m json_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_template(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mYou are a helpful assistant.\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124mExtract the presenting problem and emotional history from the following text as JSON only.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;124m}}\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Chain: prompt → LLM\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m json_chain: Runnable \u001b[38;5;241m=\u001b[39m \u001b[43mjson_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mllm_json\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Input text to test\u001b[39;00m\n\u001b[0;32m     19\u001b[0m user_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI yelled at my son again. I feel terrible like I’m failing as a parent. This has been going on for weeks.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\software downloads\\Lib\\site-packages\\langchain_core\\runnables\\base.py:575\u001b[0m, in \u001b[0;36mRunnable.__or__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__or__\u001b[39m(\n\u001b[0;32m    565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    566\u001b[0m     other: Union[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    572\u001b[0m     ],\n\u001b[0;32m    573\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RunnableSerializable[Input, Other]:\n\u001b[0;32m    574\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compose this Runnable with another object to create a RunnableSequence.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m RunnableSequence(\u001b[38;5;28mself\u001b[39m, \u001b[43mcoerce_to_runnable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\software downloads\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5930\u001b[0m, in \u001b[0;36mcoerce_to_runnable\u001b[1;34m(thing)\u001b[0m\n\u001b[0;32m   5925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunnable[Input, Output]\u001b[39m\u001b[38;5;124m\"\u001b[39m, RunnableParallel(thing))\n\u001b[0;32m   5926\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   5927\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected a Runnable, callable or dict.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5928\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInstead got an unsupported type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(thing)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5929\u001b[0m )\n\u001b[1;32m-> 5930\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected a Runnable, callable or dict.Instead got an unsupported type: <class 'google.generativeai.generative_models.GenerativeModel'>"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a ChatPromptTemplate\n",
    "json_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant.\n",
    "Extract the presenting problem and emotional history from the following text as JSON only.\n",
    "\n",
    "Text: {input}\n",
    "\n",
    "Respond in this JSON format only:\n",
    "{{\n",
    "  \"presenting_problem\": \"...\",\n",
    "  \"history\": \"...\"\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Chain: prompt → LLM\n",
    "json_chain: Runnable = json_prompt | llm_json\n",
    "\n",
    "# Input text to test\n",
    "user_input = \"I yelled at my son again. I feel terrible like I’m failing as a parent. This has been going on for weeks.\"\n",
    "\n",
    "# Streaming the response\n",
    "print(\"\\n[Streaming JSON Extraction Output]\\n\")\n",
    "stream = json_chain.stream({\"input\": user_input})\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da6c09",
   "metadata": {},
   "source": [
    "# using ChatGoogleGenerativeAI to solve error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d73d922a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Streaming JSON Extraction Output]\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"presenting_problem\": \"Yelling at son, feeling terrible and like a failure as a parent\",\n",
      "  \"history\": \"This pattern of yelling has been occurring for weeks.\"\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.runnables import Runnable\n",
    "from langchain_core.messages import AIMessageChunk\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Get API key\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Gemini LangChain wrapper\n",
    "llm_json = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # or gemini-1.5-pro\n",
    "    google_api_key=GEMINI_API_KEY,\n",
    "    temperature=0,\n",
    "   # convert_system_message_to_human=True,\n",
    "   # stream=True  # <-- important for streaming\n",
    ")\n",
    "\n",
    "# JSON extraction prompt\n",
    "json_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful assistant.\n",
    "Extract the presenting problem and emotional history from the following text as JSON only.\n",
    "\n",
    "Text: {input}\n",
    "\n",
    "Respond in this JSON format only:\n",
    "{{\n",
    "  \"presenting_problem\": \"...\",\n",
    "  \"history\": \"...\"\n",
    "}}\n",
    "\"\"\")\n",
    "\n",
    "# Runnable chain\n",
    "json_chain: Runnable = json_prompt | llm_json\n",
    "\n",
    "# Test input\n",
    "user_input = \"I yelled at my son again. I feel terrible like I’m failing as a parent. This has been going on for weeks.\"\n",
    "\n",
    "# Stream output\n",
    "print(\"\\n[Streaming JSON Extraction Output]\\n\")\n",
    "stream = json_chain.stream({\"input\": user_input})\n",
    "\n",
    "for chunk in stream:\n",
    "    if isinstance(chunk, AIMessageChunk):\n",
    "        print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf326ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
